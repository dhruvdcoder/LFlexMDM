# @package _global_
# Experiment with without shared backbone between main and aux models
# Adapted for OpenWebText (OWT)

defaults:
  - override /datamodule: safe_lflexmdm
  - override /noise_schedule: lflexmdm_kuma
  - override /model_type: lflexmdm
  - override /model: ddit_lflexmdm_kuma

per_device_batch_size: 128
global_batch_size: 2048
block_size: 256
monitored_metric: val/validity
num_dataloader_workers: 4
use_bracket_safe: true
num_unconditional_prediction_examples: 1000

# Rate head parameters
rate_head_type: scalar
rate_head_use_mlp: true
num_rate_bins: 1000
min_rate: 1.01
max_rate: 100.0
schedule_type: simplified-kuma
scalar_fn: exp

# post-hoc evaluation
post_hoc_evaluator:
  _target_: xlm.tasks.molgen.DeNovoEval
  use_bracket_safe: ${oc.select:use_bracket_safe,true}

predictor:
  _target_: lflexmdm.predictor_tau_leaping_kuma.TauLeapingPredictor
  max_steps: 1000
  top_k: null
  top_p: 0.9

loss:
  _target_: lflexmdm.loss_flexmdm_kuma.LFlexMDMLoss
  regularizer_weight: 100.0
  use_length_scale: true

global_components:
  noise_schedule:
    a_min: 2.00

datamodule:
  print_batch_fn: lflexmdm.datamodule_flexmdm.print_batch_flexmdm

model:
  min_rate: 0.0
  scalar_fn: ${oc.select:scalar_fn,softplus}

aux_model:
  scalar_fn: ${oc.select:scalar_fn,softplus}

trainer:
  max_steps: 100_000
  val_check_interval: 100000 #
  num_sanity_val_steps: 3
  check_val_every_n_epoch: null

log_predictions:
  _target_: xlm.log_predictions.LogPredictions
  fields_to_keep_in_output:
    - text
    - smiles
    - diversity
    - validity
    - uniqueness
    - qed
    - sa
    - quality
  inject_target: null
  writers:
    - file
    - logger

callbacks:
  checkpoint_every_n_steps:
    every_n_train_steps: 1000
    keep_multiple: 25 # Keep every (keep_multiple * every_n_train_steps) permanently = 1000 * 25 = 25_000

optimizer:
  lr: 0.0001

#lr_scheduler:
#  name: "constant_with_warmup"
#  num_warmup_steps: 2000
#  num_training_steps: ${trainer.max_steps}
#  monitor: "train/loss"

lr_scheduler:
  name: "cosine_with_min_lr"
  min_lr: 1e-6
  num_warmup_steps: 2000
  num_cycles: 3.0
  num_training_steps: ${trainer.max_steps}
  monitor: "train/loss"