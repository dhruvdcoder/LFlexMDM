# @package _global_
# Shared backbone configuration for FlexMDM with LoRA on aux model

# Shared transformer backbone (owned by harness, passed to both models)
backbone:
  _target_: lflexmdm.model_flexmdm_kuma.SharedTransformerBackbone
  num_embeddings: ${tokenizer:full_vocab_size}
  d_model: 768
  num_layers: 12
  nhead: 12
  padding_idx: ${tokenizer:pad_token_id}
  dim_feedforward: ${eval:${.d_model}*4}
  dropout: 0.1
  activation: "relu"
  layer_norm_eps: 1e-5
  d_cond: 128
  rotary_emb_dim: 64
  max_length: ${eval:${block_size}+${oc.select:input_block_size,0}+4}
  force_flash_attn: false

# Main model (uses shared backbone unfrozen)
model:
  _target_: lflexmdm.model_flexmdm_kuma.FlexMDMModelShared
  # backbone: injected by harness
  num_embeddings: ${tokenizer:full_vocab_size}
  d_model: 768
  d_cond: 128
  layer_norm_eps: 1e-5
  rate_head_type: ${rate_head_type}
  num_rate_bins: ${num_rate_bins}
  min_rate: ${min_rate}
  max_rate: ${max_rate}
  rate_head_use_mlp: ${rate_head_use_mlp}
  schedule_type: ${schedule_type}

# Auxiliary model (uses shared backbone frozen + LoRA)
aux_model:
  _target_: lflexmdm.model_flexmdm_kuma.FlexMDMAuxModelSharedLoRA
  # backbone: injected by harness
  d_model: 768
  dim_feedforward: ${eval:${.d_model}*4}
  d_cond: 128
  rate_head_type: ${rate_head_type}
  num_rate_bins: ${num_rate_bins}
  min_rate: ${min_rate}
  max_rate: ${max_rate}
  rate_head_use_mlp: ${rate_head_use_mlp}
  schedule_type: ${schedule_type}
  # Behavior toggles:
  # - use_lora=true, freeze_backbone=true: original behavior (frozen backbone + LoRA)
  # - use_lora=false, freeze_backbone=true: frozen backbone, no LoRA
  # - use_lora=false, freeze_backbone=false: vanilla backprop through backbone
  use_lora: true
  freeze_backbone: true
  # LoRA configuration
  lora_rank: ${oc.select:lora_rank,16}
  lora_alpha: ${oc.select:lora_alpha,16.0}
  lora_dropout: ${oc.select:lora_dropout,0.0}

tags:
  model: ddit_lflexmdm_kuma_shared


